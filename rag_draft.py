# -*- coding: utf-8 -*-
"""RAG_draft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N3YwMKfGbl01fMu5PFXYj67EU3rG3X8t

# 1. Initialization and Setup

"""## Setup"""

# Set up environment variables and API keys
import os
os.environ["LANGCHAIN_TRACING_V2"] = "True"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "2073a39bc9976b5c82a446cfc5b8def280c121f35255ee557b7f3416ba0ea3d3"
os.environ["LANGCHAIN_PROJECT"] = "personal_rag_temg"
os.environ["TAVILY_API_KEY"] = "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"

"""## Import"""

# Import required libraries
import os
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
import sqlite3
from pprint import pprint
from typing import List, Dict

# Import LangChain and related libraries
from typing_extensions import TypedDict
from langchain_together import ChatTogether, TogetherEmbeddings
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain.schema import Document
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma 
from langchain_community.document_loaders import WebBaseLoader 
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import create_retrieval_chain
from langchain.chains import RetrievalQA
from langchain_community.chat_models import ChatOpenAI
from langchain.tools import BaseTool
from langchain.agents import Tool
from langchain_community.utilities import GoogleSearchAPIWrapper
from langchain.memory import ConversationBufferMemory
from langchain_community.callbacks.manager import get_openai_callback
from langgraph.graph import END, StateGraph, START

# Additional imports
from sentence_transformers import SentenceTransformer
from langchain_community.tools.tavily_search import TavilySearchResults

#from google.colab import drive
#drive.mount('/content/drive')

"""## LLM"""

llm = ChatTogether(
    model="meta-llama/Llama-3.2-90B-Vision-Instruct-Turbo",
    temperature=0.7,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key="fc8763f0b8fdf8a3be5af42b2406145c4a1841385230110b54cda0cfd2dc3858"
)

#!pip install transformers pillow torch torchvision

# Import required modules
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
import torch

# Set device
embedding_function = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L6-v2")

# Initialize the image captioning model and processor
image_captioning_model = VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')
feature_extractor = ViTImageProcessor.from_pretrained('nlpconnect/vit-gpt2-image-captioning')
tokenizer = AutoTokenizer.from_pretrained('nlpconnect/vit-gpt2-image-captioning')
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token
# Set device (use GPU if available)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
image_captioning_model.to(device)

"""# 2. Data Preparation

## Load
"""

# Load datasets
file_path = "./merged_data.csv"
data = pd.read_csv(file_path)

# Load push notifications data
push_notifs_path = "./ph_push_notifs.xlsx"
push_notifs = pd.read_excel(push_notifs_path)

time.sleep(2)
conn = sqlite3.connect('domains.db')
cursor = conn.cursor()
cursor.execute('''
CREATE TABLE IF NOT EXISTS domains (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    domain_name TEXT NOT NULL UNIQUE,
    url TEXT NOT NULL,
    priority REAL NOT NULL
)
''')
conn.commit()

"""## Clean"""

# Clean and preprocess data
data['HOST'] = data['HOST'].str.strip().fillna('')
data['VO_TALENT'] = data['VO_TALENT'].str.strip().fillna('')
data['WIKI_EN_URL'] = data['WIKI_EN_URL'].str.strip().fillna('')
data["SERIES_NAME_ALL"] = data["SERIES_NAME_ALL"].str.lower().str.replace('(tagalog)', '').str.strip()

"""## Prepare Documents"""

# Prepare documents from push notifications
documents = []
for index, row in push_notifs.iterrows():
    content = f"Title: {row['Title']}\nMessage: {row['Message']}\nClick Rate: {row['Click Rate']}"
    metadata = {
        "Campaign ID": row["Campaign ID"],
        "Title": row["Title"],
        "Start Date": str(row["Start Date"])
    }
    doc = Document(page_content=content, metadata=metadata)
    documents.append(doc)

print(f"Number of documents created: {len(documents)}")
print(f"Sample document:\n{documents[0].page_content}")

"""## Vectorize"""

# Initialize vector store
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embedding_function,
    persist_directory="new_chroma_collection_384d"
)

retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

"""# 3. Query Processing"""

# Define prompts and parsers for query processing
prompt_rag_fusion_template = """You are a helpful assistant that generates multiple search queries based on a single input query.
Generate multiple search queries related to: {question}
Only output the queries with no preamble or explanation.
Output (4 queries):"""
prompt_rag_fusion = ChatPromptTemplate.from_template(prompt_rag_fusion_template)

# Function to generate queries
generate_queries = (
    prompt_rag_fusion
    | llm
    | StrOutputParser()
    | (lambda x: [line for line in x.split("\n") if line])
)

# Define prompt for decoding title and episode from query
decoder_prompt_template = """You are a movie and TV series expert inferring what title and which episode a certain query pertains to. Provide the title in all lowercase letters and episode as strings and output a JSON object with two keys 'title' and 'episode' with no preamble or explanation.
If there is no episode involved in the query, default it to 1.
Here is the query: {question}"""
decoder_prompt = PromptTemplate(template=decoder_prompt_template, input_variables=["question"])
decoder = decoder_prompt | llm | JsonOutputParser()

scrape_checker_prompt_template = """You are a grader assessing whether a certain document relates to a given query.
Give a binary score 'yes' or 'no' to indicate whether the content of the document is relevant to the query.
Provide the binary score as a JSON with a single key 'score' and no preamble or explanation. Be on the stringent side when grading.
Here is the document:
{documents}
And here is the query:
{query}"""
scrape_checker_prompt = PromptTemplate(
    template=scrape_checker_prompt_template,
    input_variables=["documents", "query"]
)
scrape_checker = scrape_checker_prompt | llm | JsonOutputParser()

"""# 4. Document Retrieval"""

# Function to fetch content from a Wikipedia page
def fetch_wiki_content(url):
    """Fetch content from a Wikipedia page."""
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        content = soup.find('div', {'class': 'mw-parser-output'}).get_text()
        return content.strip()
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None

# Function to retrieve documents based on query
def retrieve(state):
    """
    Retrieve documents from vectorstore or fetch from Wikipedia.
    """
    query_result = decoder.invoke({"question": state['question']})
    title = query_result['title']
    episode = query_result['episode']
    print("---RETRIEVE---")
    series_query = data.loc[
        (data["SERIES_NAME_ALL"] == title) & (data['EPS_NO'] == int(episode))
    ]
    if not series_query.empty and series_query["WIKI_EN_URL"].iloc[0]:
        content = fetch_wiki_content(series_query["WIKI_EN_URL"].iloc[0])
        if content:
            documents = [Document(page_content=content)]
            return {"documents": documents, "question": state["question"]}
    # If no content found, return empty documents
    return {"documents": [], "question": state["question"]}

#!pip install ffmpeg yt_dlp
#!pip install youtube-search-python

import ffmpeg
import os
import cv2
import numpy as np
import pandas as pd
import subprocess

import os
import subprocess
import cv2

def split_video_by_scenes(video_path, output_folder, threshold=0.3):
    # Ensure the output folder exists
    os.makedirs(output_folder, exist_ok=True)

    filename, ext = os.path.splitext(os.path.basename(video_path))
    command = [
        'ffmpeg',
        '-y',  # Overwrite output files without asking
        '-i', video_path,
        '-filter:v', f"select='gt(scene,{threshold})',showinfo",
        '-f', 'null', '-'
    ]

    # Enhanced subprocess handling with timeout
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    try:
        stdout, stderr = process.communicate(timeout=300)  # Timeout in seconds
    except subprocess.TimeoutExpired:
        process.kill()
        stdout, stderr = process.communicate()
        print(f"Process timed out: {video_path}")

    if process.returncode != 0:
        print(f"Error processing {video_path}: {stderr}")
        return []

    times = []
    for line in stderr.split('\n'):
        if 'pts_time:' in line:
            time_str = line.split('pts_time:')[1].split(' ')[0]
            times.append(float(time_str))

    video = cv2.VideoCapture(video_path)
    fps = video.get(cv2.CAP_PROP_FPS)
    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))
    times.append(total_frames / fps)
    video.release()

    split_files = []
    for i in range(len(times) - 1):
        start_time = times[i]
        end_time = times[i + 1]
        output_path = os.path.join(output_folder, f"{filename}_part{i+1}{ext}")

        # Use subprocess.run for better handling
        split_command = [
            'ffmpeg',
            '-y',
            '-i', video_path,
            '-ss', str(start_time),
            '-to', str(end_time),
            '-c', 'copy',
            output_path
        ]

        result = subprocess.run(split_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

        if result.returncode != 0:
            print(f"Error splitting {video_path} into {output_path}: {result.stderr}")
        else:
            split_files.append(output_path)

    return split_files

import os
import yt_dlp
from yt_dlp.utils import DownloadError
import csv

downloaded_videos = set()

def create_metadata_file(video_info, output_path, filename):
    metadata_content = (
        f"Title: {video_info['title']}\n"
        f"URL: {video_info['webpage_url']}\n"
        f"Duration: {video_info['duration']}\n"
        f"ID: {video_info['id']}\n"
    )

    txt_filename = os.path.splitext(filename)[0] + '.txt'
    with open(os.path.join(output_path, txt_filename), 'w', encoding='utf-8') as file:
        file.write(metadata_content)
def download_video(url, filename, output_path):
    # Ensure the output path exists
    os.makedirs(output_path, exist_ok=True)

    ydl_opts = {
        'format': 'best',  # Download the best available quality
        'outtmpl': os.path.join(output_path, filename),  # Save with the specified filename
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        # Download the video
        ydl.download([url])  # Pass the URL as a list

        # Prepare the filename to get the path of the downloaded file
        downloaded_file_path = ydl.prepare_filename(ydl.extract_info(url, download=False))

    return downloaded_file_path
# if __name__ == "__main__":
#     script_dir = os.path.dirname(os.path.abspath(__file__))
#     LINKS_FILE = os.path.join(script_dir, "assets", "extracted_video_links_movies.csv")
#     OUTPUT_PATH = os.path.join(script_dir, "assets", "videos")
#     METADATA_PATH = os.path.join(script_dir, "assets", "videodata")

#     START_INDEX = 46001
#     END_INDEX = 47000

#     download_videos_from_file(LINKS_FILE, OUTPUT_PATH, METADATA_PATH, START_INDEX, END_INDEX)

from youtubesearchpython import VideosSearch
import os
import csv

# Function to get the first YouTube video link using youtubesearchpython library
def get_youtube_link(query):
    try:
        videos_search = VideosSearch(query, limit=1)
        results = videos_search.result()
        if results['result']:
            description = ''
            return results['result'][0]['link']

        else:
            return None, None, None, None
    except Exception:
        return None, None, None, None

# split_video_by_scenes(download_video(get_youtube_link('I am not a robot viu'),'testing','./'),'./scenes')

# Install required libraries
#!pip install opencv-python moviepy scikit-image scenedetect[opencv]

"""# **5. thumbnail generation**"""

# Import necessary libraries
import cv2
import numpy as np
from moviepy.editor import VideoFileClip
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
#from google.colab import drive
# Mount Google Drive to access video files
## Stage 1 - Video Splitting
# Function to extract and visualize key frames for detected scenes
def visualize_detected_scenes(video_path, scene_list, frame_size=(160, 90), cols=5):
    cap = cv2.VideoCapture(video_path)
    key_frames = []

    # Extract the starting frame of each scene
    for scene in scene_list:
        start_frame = scene[0].get_frames()
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        ret, frame = cap.read()
        if ret:
            resized_frame = cv2.resize(frame, frame_size)
            key_frames.append(resized_frame)

    cap.release()

    # Create a montage if frames are collected
    if key_frames:
        rows = (len(key_frames) // cols) + (1 if len(key_frames) % cols != 0 else 0)
        montage = np.zeros((rows * frame_size[1], cols * frame_size[0], 3), dtype=np.uint8)

        for i, frame in enumerate(key_frames):
            row = i // cols
            col = i % cols
            montage[row * frame_size[1]:(row + 1) * frame_size[1], col * frame_size[0]:(col + 1) * frame_size[0], :] = frame

        
## Stage 2 - Clips Picking
#!pip install opencv-python moviepy scikit-image scenedetect[opencv] transformers pillow torch torchvision langchain_together
def extract_key_frames(video_path, scene_list):
    cap = cv2.VideoCapture(video_path)
    key_frames = []

    for scene in scene_list:
        start_frame = scene[0].get_frames()
        end_frame = scene[1].get_frames()
        middle_frame = (start_frame + end_frame) // 2
        cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)
        ret, frame = cap.read()
        if ret:
            key_frames.append(frame)

    cap.release()
    return key_frames

from langchain import PromptTemplate, LLMChain
from langchain.chains import SimpleSequentialChain
from langchain.prompts import PromptTemplate

prompt_template = PromptTemplate(
    input_variables=["captions_list"],
    template="""
Given the following image descriptions, select the ones that are most appealing for marketing purposes, especially those that involve kissing, crying, or action scenes. Provide the indices of the selected descriptions.

{captions_list}

Selected indices (as a comma-separated list of numbers):
"""
)

chain = prompt_template | llm

import os
import cv2
import re
from PIL import Image
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, VisionEncoderDecoderModel, ViTFeatureExtractor
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Function to select thumbnails
def select_thumbnails(series_name):
    video_path = download_video(get_youtube_link(f'{series_name} trailer'), 'testing', './')
    # Check if the video file exists
    if not os.path.exists(video_path):
        print(f"Video file for '{series_name}' not found.")
        return None, None

    # Function to detect scenes
    def detect_scenes(video_path):
        video_manager = VideoManager([video_path])
        scene_manager = SceneManager()
        scene_manager.add_detector(ContentDetector(threshold=30.0))
        video_manager.start()
        scene_manager.detect_scenes(frame_source=video_manager)
        scene_list = scene_manager.get_scene_list()
        video_manager.release()
        return scene_list

    # Function to extract key frames
    def extract_key_frames(video_path, scene_list):
        cap = cv2.VideoCapture(video_path)
        key_frames = []
        for scene in scene_list:
            start_frame = scene[0].get_frames()
            end_frame = scene[1].get_frames()
            middle_frame = (start_frame + end_frame) // 2
            cap.set(cv2.CAP_PROP_POS_FRAMES, middle_frame)
            ret, frame = cap.read()
            if ret:
                key_frames.append(frame)
        cap.release()
        return key_frames

    # Function to generate captions
    def generate_caption(image):
        pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values.to(device)
        output_ids = image_captioning_model.generate(pixel_values, max_length=16, num_beams=4)
        caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return caption

    # Function to analyze captions
    def analyze_captions(captions):
        captions_list = ""
        for idx, caption in enumerate(captions):
            captions_list += f"{idx + 1}. {caption}\n"
        prompt_template = PromptTemplate(
            input_variables=["captions_list"],
            template="""
You are assisting a marketing team to select the most appealing scenes from a video trailer for promotional purposes. The scenes should be attractive, romantic and likely to be emotionally engaging to the audience, such as those involving kissing, crying, dramatic expressions, or intense action sequences.

Below are descriptions of key frames extracted from the trailer:

{captions_list}

Based on these descriptions, rank the scenes and provide the indices of the top 5 scenes that best fit the criteria. Provide the indices as a comma-separated list in descending order of relevance.

Selected indices:
"""
        )
        chain = prompt_template | llm
        response = chain.invoke({"captions_list": captions_list})
        selected_indices_str = response.content.strip()
        indices = re.findall(r'\d+', selected_indices_str)
        selected_indices = [int(idx) - 1 for idx in indices]
        selected_indices = selected_indices[:5]
        return selected_indices

    # Detect scenes
    scene_list = detect_scenes(video_path)

    # Extract key frames
    key_frames = extract_key_frames(video_path, scene_list)

    # Generate captions for key frames
    captions = []
    for frame in key_frames:
        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        caption = generate_caption(image)
        captions.append(caption)

    # Analyze captions to get selected frame indices
    selected_indices = analyze_captions(captions)
    print("Selected frames indices:", selected_indices)

    # Retrieve the selected frames based on indices
    selected_frames = [key_frames[i] for i in selected_indices]
    selected_captions = [captions[i] for i in selected_indices]

    

    return selected_frames, selected_captions
import os

# Specify the file path
file_path = './trailer.mp4'

# Check if the file exists
if os.path.exists(file_path):
    print(f"{file_path} exists.")
else:
    print(f"{file_path} does not exist.")


import base64
from together import Together
def convert_frame_to_base64(frame):
    # Convert the frame (NumPy array) to a base64 string
    _, buffer = cv2.imencode('.jpg', frame)  # Encode frame as JPEG
    image_base64 = base64.b64encode(buffer).decode('utf-8')  # Convert to base64 string
    return image_base64 # Return as a data URL


"""# 5. Web Scraping & Filtering"""

# Domain management functions
def insert_domain(domain_name, url, priority):
    try:
        cursor.execute('''
        INSERT INTO domains (domain_name, url, priority)
        VALUES (?, ?, ?)
        ''', (domain_name, url, priority))
        conn.commit()
    except sqlite3.IntegrityError:
        print(f"Domain '{domain_name}' already exists.")

def fetch_domains():
    cursor.execute('SELECT * FROM domains')
    return cursor.fetchall()

def update_domain(domain_name, new_url=None, new_priority=None):
    updates = []
    params = []
    if new_url:
        updates.append("url = ?")
        params.append(new_url)
    if new_priority is not None:
        updates.append("priority = ?")
        params.append(new_priority)
    if updates:
        params.append(domain_name)
        update_query = f'''
        UPDATE domains
        SET {', '.join(updates)}
        WHERE domain_name = ?
        '''
        cursor.execute(update_query, params)
        conn.commit()

def delete_domain(domain_name):
    cursor.execute('''
        DELETE FROM domains WHERE domain_name = ?
    ''', (domain_name,))
    conn.commit()

# Function to perform web search and filter results
def web_search(state):
    """
    Perform web search based on the question and filter results.
    """

    print("---WEB SEARCH---")
    docs = []
    question = state["question"]
    query_result = decoder.invoke({"question": state['question']})
    title = query_result['title']
    episode = query_result['episode']
    seed_priority = 50  # Initial priority for new domains

    # Fetch existing domains and sort by priority
    domains = fetch_domains()
    domains = sorted(domains, key=lambda x: x[3], reverse=True)

    # Initialize web search tool
    web_search_tool = TavilySearchResults(
        max_results=3,
        include_answer=True,
        include_images=True,
        include_raw_content=True
    )

    # Search in prioritized domains
    for domain in domains:
        if len(docs) >= 1:
            break
        web_search_tool.include_domains = [domain[1]]
        proposed_docs = web_search_tool.invoke({"query": f'{title} episode {episode}'})

        for proposed_doc in proposed_docs:
            # Grading the relevance of the document
            grade_result = summary_checker.invoke({
                'documents': proposed_doc['content'],
                'query': f'{title} episode {episode}'
            })
            score = float(grade_result['score'])

            # Update domain priority based on score
            if score > 0.5:
                docs.append(proposed_doc)
                new_priority = domain[3] * (0.5 + score)
                update_domain(domain[1], domain[2], new_priority)
            else:
                new_priority = domain[3] * (0.5 + score)
                if new_priority <= 0.2:
                    delete_domain(domain[1])
                else:
                    update_domain(domain[1], domain[2], new_priority)

    # Search in new domains if needed
    limit = 0
    while len(docs) < 1 and limit < 20:
        web_search_tool.exclude_domains = [x[1] for x in domains]
        proposed_docs = web_search_tool.invoke({"query": f'{title} episode {episode}'})
        for proposal in proposed_docs:
            domain_name = proposal['url'].split('/')[2]
            grade_result = summary_checker.invoke({
                'documents': proposal['content'],
                'query': f'{title} episode {episode}'
            })
            score = float(grade_result['score'])

            if score > 0.5:
                docs.append(proposal)
                insert_domain(domain_name, proposal['url'], seed_priority)
            domains.append((None, domain_name, proposal['url'], seed_priority))
        limit += 1
    limit = 0
    events = []
    domains = []
    while len(events) < 3 and limit < 20:
        web_search_tool.exclude_domains = domains
        web_search_tool.include_domains = ['philstar.com','inquirer.net','mb.com.ph']
        proposed_docs = web_search_tool.invoke({"query": f'updates on {title}'})
        for proposal in proposed_docs:
            domain_name = proposal['url'].split('/')[2]
            grade_result = scrape_checker.invoke({
                'documents': proposal['content'],
                'query': f'updates on {title}'
            })
            score = grade_result['score']
            if score =='yes':
                events.append(proposal)
                insert_domain(domain_name, proposal['url'], seed_priority)
            domains.append(domain_name)
        limit += 1
    weather = []
    domains= []
    while len(weather) < 3 and limit < 20:
        web_search_tool.exclude_domains = domains
        proposed_docs = web_search_tool.invoke({"query": f'weather trends in the philippines'})
        for proposal in proposed_docs:
            domain_name = proposal['url'].split('/')[2]
            grade_result = scrape_checker.invoke({
                'documents': proposal['content'],
                'query': f'weather trends in the philippines'
            })
            score = grade_result['score']
            if score =='yes':
                weather.append(proposal)
                insert_domain(domain_name, proposal['url'], seed_priority)
            domains.append(domain_name)

        limit += 1
    holidays = []
    domains= []
    while len(holidays) < 3 and limit < 20:
        web_search_tool.exclude_domains = domains

        proposed_docs = web_search_tool.invoke({"query": f'upcoming Holidays in the philippines'})
        for proposal in proposed_docs:
            domain_name = proposal['url'].split('/')[2]
            grade_result = scrape_checker.invoke({
                'documents': proposal['content'],
                'query': f'upcoming Holidays in the philippines'
            })
            score = grade_result['score']

            if score =='yes':
                holidays.append(proposal)
                insert_domain(domain_name, proposal['url'], seed_priority)
            domains.append(domain_name)

    print(f"Web scraped documents: {docs}")
    web_results = "\n".join([d["content"] for d in docs])
    documents = state.get("documents", [])
    documents.append(Document(page_content=web_results))

    web_results = "\n".join([d["content"] for d in events])
    events = state.get("events", [])
    events.append(Document(page_content=web_results))

    web_results = "\n".join([d["content"] for d in weather])
    weather = state.get("weather", [])
    weather.append(Document(page_content=web_results))

    web_results = "\n".join([d["content"] for d in holidays])
    holidays = state.get("documents", [])
    holidays.append(Document(page_content=web_results))
    print(events[0].page_content)
    print(weather[0].page_content)
    print(holidays[0].page_content)
    return {"documents": documents, "question": question, 'events': events, 'weather':weather,'holidays':holidays}

# check = fetch_domains()
# print(check)

"""# Localization and Translation"""

#!pip install deep_translator

from deep_translator import GoogleTranslator

def google_translate(text):
    translated = GoogleTranslator(source='en', target='tl').translate(text)
    return translated

# Example usage
english_text = "Hello, this is a push notification."
tagalog_translation = google_translate(english_text)
print("Translated to Tagalog:", tagalog_translation)

#!pipi install sacremoses
import torch
from transformers import MarianMTModel, MarianTokenizer

def mt_tagalog(text):
    # Load the tokenizer and model
    model_name = f'Helsinki-NLP/opus-mt-en-tl'  # Change 'en' to your source language if needed
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)

    # Prepare the input text
    inputs = tokenizer(text, return_tensors="pt", padding=True)

    # Perform the translation
    with torch.no_grad():
        translated = model.generate(**inputs)

    # Decode the translated text
    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
    return translated_text

"""# 6. Generation"""

# Define prompt for generation
generation_prompt_template = """You are a creative marketing assistant that generates catchy and natural-sounding messages. Using knowledge from the provided context, generate messages according to the request.
Request: {question}
Incorporate the following context: {context}
Here are some examples you can refer to but not copy. {examples}
Additionally, prepend the message with a meaningful title relevant to the push notification Generate your response only in English:
Output only the messages with no preamble, explanation, title, click rate, or other unnecessary fields.
Answer:"""

generation_prompt = PromptTemplate(
    template=generation_prompt_template,
    input_variables=["question", "context", "examples"]
)


translation_prompt_template = """You are a tagalog translator that translates messages into natural sounding Tagalog. You are provided with potential translations of the text. Basing off the translations and your own knowledge, generate a translation of the original message.
the following is the original message: {push_notif}
Here are the translations: {translations}
Additionally, prepend the translation with a meaningful title relevant to the push notification in tagalog and separate it from the message using *** in the form <title>***<message> and nothing else.
Output only the final translation with no preamble, explanation, title, click rate, or other unnecessary fields.
Answer:"""

translation_prompt = PromptTemplate(
    template=translation_prompt_template,
    input_variables=["push_notif", "translations"]
)

from langchain.load import dumps, loads


def reciprocal_rank_fusion(results: list[list], k=60):
    """ Reciprocal_rank_fusion that takes multiple lists of ranked documents
        and an optional parameter k used in the RRF formula """

    # Initialize a dictionary to hold fused scores for each unique document
    fused_scores = {}
    # Iterate through each list of ranked documents
    for docs in results:
        # Iterate through each document in the list, with its rank (position in the list)
        for rank, doc in enumerate(docs):
            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)
            doc_str = dumps(doc)
            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0
            if doc_str not in fused_scores:
                fused_scores[doc_str] = 0
            # Retrieve the current score of the document, if any
            previous_score = fused_scores[doc_str]
            # Update the score of the document using the RRF formula: 1 / (rank + k)
            fused_scores[doc_str] += 1 / (rank + k)

    # Sort the documents based on their fused scores in descending order to get the final reranked results
    reranked_results = [
        (loads(doc), score)
        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    ]

    # Return the reranked results as a list of tuples, each containing the document and its fused score
    return reranked_results

retrieval_chain = generate_queries | retriever.map() | reciprocal_rank_fusion

import time
import random



# Function to generate messages
def generate(state):
    """
    Generate answer using RAG on retrieved documents.
    """

    query_result = decoder.invoke({"question": state['question']})
    title = query_result['title']
    print("---GENERATE---")

    get_youtube_link(f'{title} trailer')

    # stream = llm( messages=[
    #         {
    #             "role": "user",
    #             "content": [
    #                 {"type": "text", "text": getDescriptionPrompt},
    #                 {
    #                     "type": "image_url",
    #                     "image_url": {
    #                         "url": imageUrl,
    #                     },
    #                 },
    #             ],
    #         }
    #     ],
    #     stream=True)
    from pprint import pprint
    from typing import List

    from langchain_core.documents import Document
    from typing_extensions import TypedDict

    from langgraph.graph import END, StateGraph, START
    import time

    question = state["question"]
    documents = state["documents"]
    episode = query_result['episode']
    events = state["events"]
    weather = state["weather"]
    holidays = state["holidays"]

    series_query = data.loc[
        (data["SERIES_NAME_ALL"] == title) & (data['EPS_NO'] == int(episode))
    ]
    description = [Document(page_content=f'episode description: {series_query["EPS_DES"].values[0]}')] if "EPS_DES" in series_query and not series_query["EPS_DES"].empty else None
    hosts = [Document(page_content=f'hosts: {series_query["HOST"].values[0]}')] if "HOST" in series_query and not series_query["HOST"].empty else None
    cameo = [Document(page_content=f'cameo cast: {series_query["CAMEO_CASTS"].values[0]}')] if "CAMEO_CASTS" in series_query and not series_query["CAMEO_CASTS"].empty else None
    main = [Document(page_content=f'main cast: {series_query["MAIN_CASTS"].values[0]}')] if "MAIN_CASTS" in series_query and not series_query["MAIN_CASTS"].empty else None
    genre = [Document(page_content=f'genre: {series_query["GENRE"].values[0]}')] if "GENRE" in series_query and not series_query["GENRE"].empty else None
    informations = [info for info in [documents, episode, events, weather, holidays, description, hosts, cameo, main, genre] if info is not None]
    retrieval_chain = generate_queries | retriever.map() | reciprocal_rank_fusion

    # Generate examples using retrieval chain
    examples = retrieval_chain.invoke(question)
    notifs = ''
    for doc in examples:
        grade_result = relevance_checker.invoke({'documents': doc[0].page_content, 'query': title})
        if grade_result['score'] == 'yes':
            notifs += doc[0].page_content + '\n\n'
    print("Retrieved notifications:", notifs)

    # Generate final messages
    generation = generation_prompt | llm | StrOutputParser()
    generation_result = []
    for i in range(5):
        info = random.sample(informations, random.randint(1,2))
        flattened_info = [doc for sublist in info for doc in sublist if isinstance(doc, Document)]
        if len(flattened_info)<=0:
          i-=1
          continue
        print( "\n".join([doc.page_content for doc in flattened_info]),'\n')
        en_msg = generation.invoke({
            "context": "\n".join([doc.page_content for doc in flattened_info]),
            "question": question,
            "examples": notifs
        })
        en_msg_parts = [part for part in en_msg.split('*') if part]
        en_title = en_msg_parts[0]
        push_notif = en_msg_parts[-1]
        google_translate_output = google_translate(push_notif)
        mt_output = mt_tagalog(push_notif)
        translations = "\n".join([google_translate_output,mt_output])
        translate = translation_prompt | llm | StrOutputParser()
        tl_msg = translate.invoke({
            'push_notif':push_notif,
            'translations':translations
        })
        tl_parts = [part for part in tl_msg.split('*') if part]
        tl_title = tl_parts[0]
        tl_msg = tl_parts[1]
        entry = Suggestion(
        eng_title=en_title,
        eng_description=push_notif.strip(),
        tag_title=tl_title,
        tag_description=tl_msg.strip()
    )
        generation_result.append(entry)

    selected_frames, selected_captions = select_thumbnails(title)
    thumbnails = [convert_frame_to_base64(x) for x in selected_frames]
    return {"documents": documents, "question": question, "generation": generation_result, 'thumbnails':thumbnails}


"""# Grading & Feedback"""

# Define prompts and parsers for grading
summary_checker_prompt_template = """You are a grader assessing whether a certain document is a summary, review, or synopsis pertaining to some movie, series, or title.
Grade it based on how relevant the summary is to the query. Give a float from 0 to 1 inclusive, where 1 means it is a very good summary of the query.
Provide the float number as a JSON with a single key 'score' and no preamble or explanation.
Here is the document:
{documents}
And here is the query:
{query}"""
summary_checker_prompt = PromptTemplate(
    template=summary_checker_prompt_template,
    input_variables=["documents", "query"]
)
summary_checker = summary_checker_prompt | llm | JsonOutputParser()

relevance_checker_prompt_template = """You are a grader assessing whether a certain push notification relates to a given query.
Give a binary score 'yes' or 'no' to indicate whether the push message is relevant to the query.
You do not need to be stringent with how relevant it is. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.
Here is the document:
{documents}
And here is the query:
{query}"""
relevance_checker_prompt = PromptTemplate(
    template=relevance_checker_prompt_template,
    input_variables=["documents", "query"]
)
relevance_checker = relevance_checker_prompt | llm | JsonOutputParser()

# **Workflow Implementation**

# Define state class



import uuid
import json

class Suggestion:
    def __init__(self, eng_title, eng_description, tag_title, tag_description, liked=False):
        self.id = str(uuid.uuid4())
        self.eng_title = eng_title
        self.eng_description = eng_description
        self.tag_title = tag_title
        self.tag_description = tag_description
        self.liked = liked

    def to_dict(self):
        return {
            "id": self.id,
            "eng_title": self.eng_title,
            "eng_description": self.eng_description,
            "tag_title": self.tag_title,
            "tag_description": self.tag_description,
            "liked": self.liked
        }
    
class GraphState(TypedDict):
    question: str
    generation: List[Suggestion]
    web_search: str
    documents: List[Document]
    events: List[Document]
    weather: List[Document]
    holidays: List[Document]
    thumbnails: List[str]

# Define the workflow nodes
def route_question(state):
    """
    Route question to web search or RAG.
    """
    print("---ROUTE QUESTION---")
    query_result = decoder.invoke({"question": state['question']})
    title = query_result['title']
    episode = int(query_result['episode'])
    series_query = data.loc[
        (data["SERIES_NAME_ALL"] == title) & (data['EPS_NO'] == episode)
    ]
    if not title or series_query.empty or not series_query["WIKI_EN_URL"].iloc[0]:
        print("---ROUTE QUESTION TO WEB SEARCH---")
        return "websearch"
    else:
        print("---ROUTE QUESTION TO RAG---")
        return "retrieve"

def decide_to_generate(state):
    """
    Decide whether to generate an answer or perform web search.
    """
    time.sleep(2)
    print("---ASSESS GRADED DOCUMENTS---")
    web_search = state.get("web_search", "no")
    if web_search.lower() == "yes":
        print("---DECISION: INCLUDE WEB SEARCH---")
        return "websearch"
    else:
        print("---DECISION: GENERATE---")
        return "generate"

def grade_generation(state):
    """
    Grade the generated messages.
    """
    time.sleep(2)
    # For simplicity, we assume the generation is useful
    return "useful"

# Build the workflow graph
workflow = StateGraph(GraphState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("websearch", web_search)
workflow.add_node("generate", generate)

workflow.add_conditional_edges(
    START,
    route_question,
    {
        "retrieve": "retrieve",
        "websearch": "websearch",
    },
)
workflow.add_edge("retrieve", "generate")
workflow.add_edge("websearch", "generate")
workflow.add_conditional_edges(
    "generate",
    grade_generation,
    {
        "useful": END,
        "not useful": "websearch",
    },
)

# Compile and run the workflow
script = workflow.compile()




# masking_template = """ You are a Filipino local. Use any filipino slang or colloquialism in a sentence
# Answer:"""
# masker_prompt = PromptTemplate(
#     template=masking_template,
#     input_variables=["push"]
# )

# mask = masker_prompt | llm | StrOutputParser()
# print(mask.invoke({'push':'Sino ang nagsabi na ang mga robot ay hindi maaaring magmahal? Alamin kung ano ang mangyayari kapag nahulog ang isang siyentipiko sa isang robot sa I Am Not a Robot! Panoorin ngayon!'}))

# Step 1: Install Flask and ngrok
#!pip install Flask
#!pip install pyngrok

# from flask import Flask, jsonify

# app = Flask(__name__)

# @app.route('/')
# def home():
#   return jsonify(message='this is a sample response')

# @app.route('/api/data')
# def get_data():
#     return jsonify(data={"key": "value", "another_key": "another_value"})

# # Step 3: Run the Flask app with ngrok
# from pyngrok import ngrok
# # Set up a tunnel to the localhost
# auth_token = '2ohbdV7CJIW7F4CufL4mRh5zGi3_7116SqjkXmxbHgUST5N4q'
# ngrok.set_auth_token(auth_token)

# # Set up a tunnel to the localhost
# public_url = ngrok.connect(5000)
# print(" * ngrok tunnel \"{}\" -> \"http://127.0.0.1:5000\"".format(public_url))

# # Run the Flask app
# app.run(port=5000)



# Create FastAPI app

import nest_asyncio
from fastapi import FastAPI, Query
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from pyngrok import ngrok
from pprint import pprint

# Ngrok setup
# auth_token = '2ohmxoLcLP2MoOrg3bXIgm9CwTB_ajJx2smAazJAnNpzS9Xf'
# ngrok.set_auth_token(auth_token)
# public_url = ngrok.connect(5000)
# print(" * ngrok tunnel \"{}\" -> \"http://127.0.0.1:5000\"".format(public_url))

app = FastAPI()

origins = [
    "http://localhost.tiangolo.com",
    "https://localhost.tiangolo.com",
    "http://localhost",
    "http://localhost:8080",
    'http://10.89.120.248:3003/'
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True,
    allow_methods=["*"],  # Allows all methods
    allow_headers=["*"],  # Allows all headers
)
@app.get("/")
async def home(query: str = Query("Generate a push notification for the series I am not a robot using the provided information. Don't number the outputs")):
    inputs = {'question': query}
    print('received')
    # Assuming `script.stream(inputs)` is defined and returns the output
    # Simulating the output for demonstration
    output = {"generation": f"Processed query: {query}"}  # Replace with actual processing logic

    # Print finished running keys
    for key in output.keys():
        pprint(f"Finished running: {key}")
    
    # Return the generated message
    return JSONResponse(content={"message": output["generation"]})

@app.get("/api/data")
async def get_data():
    return JSONResponse(content={"data": {"key": "value", "another_key": "another_value"}})

   
uvicorn.run(app, host="0.0.0.0", port=8001)


